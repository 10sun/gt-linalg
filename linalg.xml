<?xml version="1.0" encoding="UTF-8"?>

<!--********************************************************************
Copyright 2017 Georgia Institute of Technology

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled "GNU
Free Documentation License"
*********************************************************************-->

<!-- USE XINCLUDE SWITCH ON XSLTPROC -->

<mathbook xmlns:xi="http://www.w3.org/2001/XInclude" xml:lang="en-US">

  <!-- ISBN, website, other metadata -->
  <xi:include href="./bookinfo.xml" />

  <book xml:id="index">
    <title>Introduction to Linear Algebra</title>

    <!-- Title Page, Preface, etc. -->
    <xi:include href="./frontmatter.xml" />

    <!-- Chapters -->
    <xi:include href="./overview.xml" />

    <chapter xml:id="chap-algebra">
      <title>Systems of Linear Equations: Algebra</title>

      <introduction>
        <p>
          In this Chapter, we will introduce <em>systems of linear equations</em>, the class of equations whose study forms the subject of linear algebra.  We will present a procedure, called <em>row reduction</em>, for finding all solutions of a system of linear equations.  You will see how to express all solutions of a system of linear equations in a unique way using the <em>parametric form</em> of the general solution.
        </p>
      </introduction>

      <xi:include href="./systems-eqns.xml" />
      <xi:include href="./row-reduction.xml" />
      <xi:include href="./parametric-form.xml" />
    </chapter>

    <chapter xml:id="chap-geometry">
      <title>Systems of Linear Equations: Geometry</title>

      <introduction>
        <p>
          This Chapter is devoted to the geometric study of two objects:
          <ol>
            <li>the solution set of a system of linear equations, and</li>
            <li>the set of all constants that makes a particular system consistent.</li>
          </ol>
          These objects are related in a beautiful way by the Rank Theorem in <xref ref="rank-thm"/>.  We will develop a large list of vocabulary words that we will use to describe these objects: vectors, spans, linear independence, subspaces, dimension, coordinate systems, etc.  We will use these concepts to give a precise geometric description of the solution set of any system of equations.  We will also learn how to express systems of equations more simply using matrix equations and non-standard coordinate systems.
        </p>
      </introduction>

      <xi:include href="./vectors.xml" />
      <xi:include href="./spans.xml" />
      <xi:include href="./matrixeq.xml" />
      <xi:include href="./solnsets.xml" />
      <xi:include href="./linindep.xml" />
      <xi:include href="./subspaces.xml" />
      <xi:include href="./dimension.xml" />
      <xi:include href="./b-coordinates.xml" />
      <xi:include href="./rank-thm.xml" />
    </chapter>

    <chapter xml:id="chap-matrices">
      <title>Linear Transformations and Matrix Algebra</title>

      <introduction>
        <p>
          In this Chapter, we are concerned with understanding the geometry of matrices as functions.  Namely, we consider the equation <m>b = Ax</m> as a function with independent variable <m>x</m> and dependent variable <m>b</m>, and we draw pictures accordingly.  We spend some time studying transformations in the abstract, and asking questions about a transformation, like whether it is one-to-one and/or onto.  We then present matrix multiplication and matrix inversion as special cases of composition and inversion of transformations, respectively.  This leads to the study of <em>matrix algebra</em>: that is, to what extent one can do arithmetic with matrices in the place of numbers.
        </p>
      </introduction>

      <xi:include href="./matrix-trans.xml" />
      <xi:include href="./one-one-onto.xml" />
      <xi:include href="./linear-trans.xml" />
      <xi:include href="./matrix-mult.xml" />
      <xi:include href="./matrix-inv.xml" />
    </chapter>

    <chapter xml:id="chap-determinant">
      <title>Determinants</title>
      <introduction>
        <p>
          We begin by recalling the three primary goals of this book:
          <ol>
            <li>Solve the matrix equation <m>Ax=b</m>.</li>
            <li>Solve the matrix equation <m>Ax=\lambda x</m>, where <m>\lambda</m> is a number.</li>
            <li>Approximately solve the matrix equation <m>Ax=b</m>.</li>
          </ol>
          At this point we have said all that we will say about the first goal.  This Chapter belongs primarily to the second main topic.
        </p>
        <p>
          The <em>determinant</em> of a square matrix <m>A</m> is a number <m>\det(A)</m>.  In <xref ref="determinants-definitions-properties"/>, we will define the determinant in terms of its behavior with respect to row operations.  The determinant satisfies many wonderful properties: for instance, <m>\det(A) \neq 0</m> if and only if <m>A</m> is invertible.  We will discuss some of these properties in <xref ref="determinants-definitions-properties"/> as well.
        </p>
        <p>
          In <xref ref="determinants-cofactors"/>, we will give a recursive formula for the determinant of a matrix.  This formula is very useful, for instance, when taking the determinant of a matrix with unknown entries; this will be important in <xref ref="chap-eigenvalues"/>.
        </p>
        <p>
          Finally, in <xref ref="determinants-volumes"/>, we will relate determinants to volumes.  This gives a geometric interpretation for determinants, and explains why the determinant is defined the way it is.  This interpretation of determinants is a crucial ingredient in the change-of-variables formula in multivariable calculus.
        </p>

      </introduction>
      <xi:include href="./determinant-definitions-properties.xml" />
      <xi:include href="./determinant-cofactors.xml" />
      <xi:include href="./determinant-volume.xml" />

    </chapter>

    <chapter xml:id="chap-eigenvalues">
      <title>Eigenvalues and Eigenvectors</title>

      <introduction>
        <p>
          This Chapter constitutes the core of any first course on linear algebra: eigenvalues and eigenvectors play a crucial role in most real-world applications of the subject.  After defining eigenvalues and eigenvectors and showing how to compute them, we will study the geometry of matrices using diagonalization and complex eigenvalues.  We also spend <xref ref="stochastic-matrices"/> presenting a common kind of application of eigenvalues and eigenvectors to real-world problems, including searching the Internet using Google<rsq/>s PageRank algorithm.
        </p>
      </introduction>

      <xi:include href="./eigenvectors.xml" />
      <xi:include href="./charpoly.xml" />
      <xi:include href="./similarity.xml" />
      <xi:include href="./diagonalization.xml" />
      <xi:include href="./stochastic.xml" />
      <xi:include href="./cplx-eigenvals.xml" />

    </chapter>

    <chapter xml:id="chap-orthogonality">
      <title>Orthogonality</title>

      <introduction>
        <p>
          Let us recall one last time the structure of this book:
          <ol>
            <li>Solve the matrix equation <m>Ax=b</m>.</li>
            <li>Solve the matrix equation <m>Ax=\lambda x</m>, where <m>\lambda</m> is a number.</li>
            <li>Approximately solve the matrix equation <m>Ax=b</m>.</li>
          </ol>
          We have now come to the third goal.
        </p>
        <p>
          Finding approximate solutions of equations generally requires computing the closest vector on a subspace from a given vector.  This becomes an <em>orthogonality</em> problem: one needs to know which vectors are perpendicular to the subspace.  First we will define orthogonality and learn to find orthogonal complements of subspaces in <xref ref="dot-product"/> and <xref ref="orthogonal-complements"/>.  The core of this chapter is <xref ref="projections"/>, in which we discuss <em>the orthogonal projection</em> of a vector onto a subspace; this is a method of calculating the closest vector on a subspace to a given vector.  These calculations become easier in the presence of an orthogonal set, as we will see in <xref ref="orthogonal-sets"/>.  Finally in <xref ref="least-squares"/> we will present the least-squares method of approximately solving systems of equations, and we will give applications to data modeling.
        </p>
      </introduction>

      <xi:include href="./innerprod.xml" />
      <xi:include href="./orthocomp.xml" />
      <xi:include href="./projections.xml" />
      <xi:include href="./orthosets.xml" />
      <xi:include href="./leastsquares.xml" />

    </chapter>

    <xi:include href="./backmatter.xml" />
  </book>

</mathbook>
