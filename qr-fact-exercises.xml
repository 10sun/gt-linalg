<?xml version="1.0" encoding="UTF-8"?>
<!--********************************************************************
Copyright 2017 Georgia Institute of Technology

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation.  A copy of
the license is included in gfdl.xml.
*********************************************************************-->

<exercises>
    <exercise>
        <statement>
            <p>If <m>A</m> is a matrix with independent columns, explain step by step how to find the QR factorization of <m>A</m>.</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>If you know the QR factorization of a matrix <m>A</m>, how would you find the least square solutions of <m>Ax=b</m>? If <m>A</m> is <m>n \times n</m>, how many operations would it take?</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>Consider the matrix
                <m>A=\mat{1 2 2;1 0 4; 0 1 2}</m>
                <ol>
                    <li>Find an orthogonal basis for <m>\Col(A)</m>.</li>
                    <li>Find a QR factorization of <m>A</m>.</li>
                </ol>
            </p>
        </statement>
        <answer>
            <p>
                <ol>
                    <li>Let
                        <md>
                            <mrow>
                                v_1 = \vec{1 1 0}
                                \qquad
                                v_2 = \vec{2 0 1}
                                \qquad
                                v_3 = \vec{2 4 2}
                            </mrow>
                        </md>
                        be the columns of <m>A</m>. We will perform Gram–Schmidt on <m>\left\{v_1, v_2, v_3\right\}</m>. Let
                        <md>
                            <mrow>
                                u_1 = v_1 = \vec{1 1 0}
                            </mrow>
                            <mrow>
                                u_2 = v_2 - \frac{v_2 \cdot u_1}{u_1 \cdot u_1}u_1 = \vec{2 0 1} - \frac{2}{2}\vec{1 1 0} = \vec{1 -1 1}
                            </mrow>
                            <mrow>
                                u_3 = v_3 - \frac{v_3 \cdot u_1}{u_1 \cdot u_1}u_1 - \frac{v_3 \cdot u_2}{u_2 \cdot u_2}u_2= \vec{2 4 2} - \frac{6}{2}\vec{1 1 0} - \frac{0}{3}\vec{1 -1 1} = \vec{-1 1 2}
                            </mrow>
                        </md>
                        An orthogonal basis for <m>\Col(A)</m> is
                        <me>
                            \left\{u_1, u_2, u_3\right\} = \left\{ \vec{1 1 0}, \vec{1 -1 1}, \vec{-1 1 2}\right\}
                        </me>.
                        (Actually, <m>\Col(A)=\R^3</m>, so the standard basis <m>e_1, e_2, e_3</m> is also an orthogonal basis of <m>\Col(A)</m>. However, we still need to do Gram–Schmidt for part (b).)
                    </li>
                    <li>Solving for <m>v_1, v_2, v_3</m> in terms of <m>u_1, u_2, u_3</m> above, we get
                        <me>\syseq{v_1 = u_1; v_2 = u_1 + u_2; v_3 = 3u_1 + u_3}</me>
                        In matrix form,
                        <me>\mat{| | |; v_1 v_2 v_3; | | |} = \mat{| | |; u_1 u_2 u_3; | | |}\mat{1 1 3; 0 1 0;0 0 1}</me>
                        Hence <m>A=\hat Q \hat R</m>, where
                        <md>
                            <mrow>
                                \hat Q = \mat{| | |; u_1 u_2 u_3; | | |} = \mat{1 1 -1; 1 -1 1; 0 1 2} \qquad \text{and} \qquad \hat R = \mat{1 1 3;0 1 0;0 0 1}
                            </mrow>
                        </md>
                        We scale the columns of <m>\hat Q</m> to obtain a matrix <m>Q</m> with orthonormal columns, and we scale the rows of <m>\hat R</m> by the opposite factor, to obtain <m>A=QR</m> where
                        <md>
                            <mrow>
                                Q = \mat{
                                \frac{1}{\sqrt{2}} \frac{1}{\sqrt{3}} -\frac{1}{\sqrt{6}};
                                \frac{1}{\sqrt{2}} -\frac{1}{\sqrt{3}} \frac{1}{\sqrt{6}};
                                0 \frac{1}{\sqrt{3}} \frac{2}{\sqrt{6}}
                                }
                                \qquad
                                \text{and}
                                \qquad
                                R = \mat{
                                \sqrt{2} \sqrt{2} 3\sqrt{2};
                                0 \sqrt{3} 0;
                                0 0 \sqrt{6}
                                }
                            </mrow>
                        </md>.
                    </li>
                </ol>
            </p>
        </answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] QR factorization of the matrix A (e.g. via Gram-Schmidt) yields A=QR, where
    			<me>Q=\mat{\frac{1}{\sqrt{2}} 0 \frac{1}{2}; 0 \frac{1}{\sqrt{2}} \frac{1}{2}; -\frac{1}{\sqrt{2}} 0 \frac{1}{2}; 0 -\frac{1}{\sqrt{2}} \frac{1}{2}}, R=\mat{1 2 0; \star, 1 2;\star, \star, 2}</me>
    			<ol>
    				<li>Which columns of <m>A</m> were orthogonal to begin with, if any?</li>
    				<li>What is the orthogonal projection <m>p</m> of the vector <m>b=\vec{4 0 0 0}</m> onto <m>\Col(A)</m>?</li>
    				<li>If we are minimizing <m>\|Ax-b\|</m> (i.e. solving the least-square problem) for <m>b=\vec{4 0 0 0}</m>, you should be able to quickly get an upper-triangular system of equations <m>U\hat x = c</m> for the least-square solution <m>\hat x</m>. What are the upper-triangular matrix <m>U</m> and the right-hand-side vector <m>c</m>?</li>
    			</ol>
    		</p>
    	</statement>
    	<answer>
    		<p>
    			<ol>
    				<li>Call the columns of <m>Q</m> <m>q_1</m>, <m>q_2</m>, <m>q_3</m> and the columns of <m>A</m> <m>a_1</m>, <m>a_2</m>, <m>a_3</m>. The matrix factorization <m>A=QR</m> with <m>R</m> the matrix given above then just means that <m>a_1=q_1</m>, 
    				<m>a_2=2q_1 + q_2</m>, <m>a_3= 2q_2+2q_3</m>. The <m>q_i</m> are orthonormal, so the dot products of the columns of <m>A</m> are the same as the dot products of the columns of <m>R</m>! Only the first and third columns of <m>R</m> are orthogonal, so only the first and thirs columns of <m>A</m> are orthogonal to begin with.</li>
    				<li>As the <m>q_i</m> from <m>(1)</m> form an orthonormal basis for <m>\Col(A)</m>, the orthogonal projection of <m>b</m> onto <m>\Col(A)</m> is
    					<md>
    						<mrow>
    							(q_1 q_1^T + q_2 q_2^T + q_3q_3^T)b = (4/\sqrt{2})q_1+(4/2)q_3
    						</mrow>
    						<mrow>
    							\qquad = (2,0,-2,0)+(1,1,1,1) = (3,1,-1,1)
    						</mrow>
    					</md>
    					</li>
    				<li>The normal equations must be satisfied, i.e. <m>A^T A\hat x = A^T b</m>. As <m>A=QR</m>, we have <m>A^T A=R^T Q^T QR=R^T IR=R^T R</m> becuase <m>Q</m> is orthogonal, so the normal equations simplify to <m>R^TR\hat x = A^T b = R^T Q^T b</m>. The matrix <m>R^T</m> is 
    				invertible, so we can cancel it from each side, giving <m>R\hat x = Q^T b</m>. So <m>U=R</m> and <m>c=Q^T b</m> do the trick. Concretely, <m>U=R</m> is given above, and <m>c=Q^T b</m> is <m>4</m> times the transpose of the first row of <m>Q</m>, given by
    					<me>Q^T b = \vec{\frac{4}{\sqrt{2}} 0 2}</me>
    				</li>
    				
    			</ol>
    		</p>
    	</answer>
    </exercise>
</exercises>
