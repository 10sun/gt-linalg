<?xml version="1.0" encoding="UTF-8"?>
<!--********************************************************************
Copyright 2017 Georgia Institute of Technology

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation.  A copy of
the license is included in gfdl.xml.
*********************************************************************-->
<exercises>
    <exercise>
        <statement>
            <p>Explain why any orthonormal collection of vectors is linearly independent.</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>How does one compute the coordinates of a vector <m>v</m> in an orthonormal basis <m>u_1,\ldots,u_n</m>? Explain why this procedure is generally more efficient than finding coordinates in an non-orthonormal basis. (Caution: any vector in an orthonormal basis <m>u_1, \ldots, u_n</m> has length <m>1</m>, so in the formula for the <m>i</m>th coordinate of <m>v</m> you need not divide by <m>u_i \cdot u_i</m>)</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>Explain step by step how to find an orthonormal basis in a subspace spanned by vectors <m>X_1,\ldots,X_n</m>. (Here <m>X_1,\ldots,X_m</m> need not be linearly independent.)</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>Every subspace of <m>\R^n</m> admits an orthogonal basis:
                <ol>
                    <li>True</li>
                    <li>False</li>
                </ol>
            </p>
        </statement>
        <answer>
            <p>1 (true). Take any basis, and apply Gram–Schmidt.</p>
        </answer>
    </exercise>
    <exercise>
        <statement>
            <p>Let <m>x</m> and <m>y</m> be nonzero orthogonal vectors in <m>\R^n</m>. Which of the following are true?
                <ol>
                    <li><m>x \cdot y =0</m></li>
                    <li><m>\| x-y \|^2 = \|x\|^2 + \|y\|^2</m></li>
                    <li><m>proj_{\Span(x)}(y)=0</m></li>
                    <li><m>proj_{\Span(y)}(x)=0</m></li>
                </ol>
            </p>
        </statement>
        <answer>
            <p>1,2,3,4.</p>
        </answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] What is the projection of the vector <m>\vec{1 1 1} \in \R^3</m> onto the plane <m>3x-4y+z=0</m>?</p>
    	</statement>
    	<answer>
    		<p>The vector <m>\vec{1 1 1}</m> lies on the plane <m>3x-4y+z=0</m>, so its projection onto that plane is simply <m>\vec{1 1 1}</m> again.</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] Suppose <m>\{v_1, \ldots, v_k\}</m> is an orthonormal set of vectors in <m>\R^n</m>. What happens when you apply Gram-Schmidt process to this set? Why?</p>
    	</statement>
    	<answer>
    		<p>The Gram-Schmidt procedure does not modify orthormal sets of vectors <m>\{v_1, ldots, v_k\}</m> in <m>\R^n</m>. Let us prove this statement by induction on <m>k</m>. If <m>k=1</m>, it is obvious. Consider an orthonormal set <m>\{v_1, \ldots, v_{k+1}\}</m> of <m>k+1</m> vectors in <m>\R^n</m>. We first apply GS to the set <m>\{v_1, \ldots, v_k\}</m>. By the induction 
    		hypothesis, this set stays unchanged. Then we compute to the orthogonal projection of <m>v_{k+1}</m> onto the space spanned by <m>v_1, ldots, v_k</m>. Since <m>v_{k+1}</m> is orthogonal to <m>v_i</m> for <m>i=1 \ldots k</m>, this projection is <m>0</m>. Therefore, GS procedure does not change the set <m>\{v_1, \ldots, v_{k+1}\}</m>.</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW]
    			<ol>
    				<li>Apply Gram-Schmidt to the following vectors in <m>\R^3</m>:
    					<me>\vec{1,2,0}, \vec{8 1 -6}, \vec{0 0 1}</me>
    				</li>
    				<li>Explain why the Gram–Schmidt process always fails (tries to divide by <m>0</m>) on an <m>m \times n</m> matrix <m>A</m> if <m>\dim(\Col(A)) \lt n</m>.</li>
    				<li>Does the Gram–Schmidt process always succeed (never divides by <m>0</m>) if <m>\dim(\Col(A)) = n</m>?</li>
    			</ol>
    		</p>
    	</statement>
    	<answer>
    		<p>
    			<ol>
    				<li>Let
    					<me>x_1=\vec{1 2 0}, x_2=\vec{8 1 -6}, x_3=\vec{0 0 1}</me>
    					Then, 
    					<md>
    						<mrow>
    							v_1 = x_1 = \vec{1 2 0}
    						</mrow>
    						<mrow>
    							v_2 = x_2 - \dfrac{x_2^T v_1}{v_1^T v_1}v_1 = \vec{8 1 -6} - \frac{10}{5}\vec{1 2 0}=\vec{6 -3 -6}
    						</mrow>
    						<mrow>
    							v_3 = x_3 - \dfrac{x_3^T v_1}{v_1^T v_1}v_1 - \dfrac{x_3^T v_2}{v_2^T v_2}v_2= \vec{0 0 1} - 0\vec{1 2 0} - \dfrac{-6}{81}\vec{6 -3 -6} = \dfrac{1}{9}\vec{4 -2 5}
    						</mrow>
    					
    					</md>
    					Now normalize the vectors
    					<md>
    						<mrow>
    							v_1 = \dfrac{v_1}{\|v_1\|} = \dfrac{1}{\sqrt{5}}\vec{1 2 0}
    						</mrow>
    						<mrow>
    							v_2 = \dfrac{v_2}{\|v_2\|} = \dfrac{1}{\sqrt{81}}\vec{6 -3 -6} = \dfrac{1}{3}\vec{2 -1 -2}
    						</mrow>
    						<mrow>
    							v_3 = \dfrac{v_3}{\|v_3\|} = \dfrac{1}{3\sqrt{5}}\vec{4 -2 5}
    						</mrow>
    					</md>
    				</li>
    				<li>If <m>A</m> is <m>m \times n</m> and <m>\dim(\Col(A)) \lt n</m>, then the columns of <m>A</m> are linearly dependent. When we project off previous columns, we are going to end up with a zero vector which is impossible to normalize (without dividing by <m>0</m>).</li>
    				<li>Yes, the columns are linearly independent. As we project off the previous columns (and do linear combinations), we can never end up with zero column.</li>
    			</ol>
    		</p>
    	</answer>
    </exercise>
</exercises>
