<?xml version="1.0" encoding="UTF-8"?>
<!--********************************************************************
Copyright 2017 Georgia Institute of Technology

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation.  A copy of
the license is included in gfdl.xml.
*********************************************************************-->
<exercises>
    <exercise>
        <statement>
            <p>Explain why any orthonormal collection of vectors is linearly independent.</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>How does one compute the coordinates of a vector <m>v</m> in an orthonormal basis <m>u_1,\ldots,u_n</m>? Explain why this procedure is generally more efficient than finding coordinates in an non-orthonormal basis. (Caution: any vector in an orthonormal basis <m>u_1, \ldots, u_n</m> has length <m>1</m>, so in the formula for the <m>i</m>th coordinate of <m>v</m> you need not divide by <m>u_i \cdot u_i</m>)</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>Explain step by step how to find an orthonormal basis in a subspace spanned by vectors <m>X_1,\ldots,X_n</m>. (Here <m>X_1,\ldots,X_m</m> need not be linearly independent.)</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>Every subspace of <m>\R^n</m> admits an orthogonal basis:
                <ol>
                    <li>True</li>
                    <li>False</li>
                </ol>
            </p>
        </statement>
        <answer>
            <p>1 (true). Take any basis, and apply Gram–Schmidt.</p>
        </answer>
    </exercise>
    <exercise>
        <statement>
            <p>Let <m>x</m> and <m>y</m> be nonzero orthogonal vectors in <m>\R^n</m>. Which of the following are true?
                <ol>
                    <li><m>x \cdot y =0</m></li>
                    <li><m>\| x-y \|^2 = \|x\|^2 + \|y\|^2</m></li>
                    <li><m>proj_{\Span(x)}(y)=0</m></li>
                    <li><m>proj_{\Span(y)}(x)=0</m></li>
                </ol>
            </p>
        </statement>
        <answer>
            <p>1,2,3,4.</p>
        </answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] What is the projection of the vector <m>\vec{1 1 1} \in \R^3</m> onto the plane <m>3x-4y+z=0</m>?</p>
    	</statement>
    	<answer>
    		<p>The vector <m>\vec{1 1 1}</m> lies on the plane <m>3x-4y+z=0</m>, so its projection onto that plane is simply <m>\vec{1 1 1}</m> again.</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] Suppose <m>\{v_1, \ldots, v_k\}</m> is an orthonormal set of vectors in <m>\R^n</m>. What happens when you apply Gram-Schmidt process to this set? Why?</p>
    	</statement>
    	<answer>
    		<p>The Gram-Schmidt procedure does not modify orthormal sets of vectors <m>\{v_1, ldots, v_k\}</m> in <m>\R^n</m>. Let us prove this statement by induction on <m>k</m>. If <m>k=1</m>, it is obvious. Consider an orthonormal set <m>\{v_1, \ldots, v_{k+1}\}</m> of <m>k+1</m> vectors in <m>\R^n</m>. We first apply GS to the set <m>\{v_1, \ldots, v_k\}</m>. By the induction 
    		hypothesis, this set stays unchanged. Then we compute to the orthogonal projection of <m>v_{k+1}</m> onto the space spanned by <m>v_1, ldots, v_k</m>. Since <m>v_{k+1}</m> is orthogonal to <m>v_i</m> for <m>i=1 \ldots k</m>, this projection is <m>0</m>. Therefore, GS procedure does not change the set <m>\{v_1, \ldots, v_{k+1}\}</m>.</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW]
    			<ol>
    				<li>Apply Gram-Schmidt to the following vectors in <m>\R^3</m>:
    					<me>\vec{1,2,0}, \vec{8 1 -6}, \vec{0 0 1}</me>
    				</li>
    				<li>Explain why the Gram–Schmidt process always fails (tries to divide by <m>0</m>) on an <m>m \times n</m> matrix <m>A</m> if <m>\dim(\Col(A)) \lt n</m>.</li>
    				<li>Does the Gram–Schmidt process always succeed (never divides by <m>0</m>) if <m>\dim(\Col(A)) = n</m>?</li>
    			</ol>
    		</p>
    	</statement>
    	<answer>
    		<p>
    			<ol>
    				<li>Let
    					<me>x_1=\vec{1 2 0}, x_2=\vec{8 1 -6}, x_3=\vec{0 0 1}</me>
    					Then, 
    					<md>
    						<mrow>
    							v_1 = x_1 = \vec{1 2 0}
    						</mrow>
    						<mrow>
    							v_2 = x_2 - \dfrac{x_2^T v_1}{v_1^T v_1}v_1 = \vec{8 1 -6} - \frac{10}{5}\vec{1 2 0}=\vec{6 -3 -6}
    						</mrow>
    						<mrow>
    							v_3 = x_3 - \dfrac{x_3^T v_1}{v_1^T v_1}v_1 - \dfrac{x_3^T v_2}{v_2^T v_2}v_2= \vec{0 0 1} - 0\vec{1 2 0} - \dfrac{-6}{81}\vec{6 -3 -6} = \dfrac{1}{9}\vec{4 -2 5}
    						</mrow>
    					
    					</md>
    					Now normalize the vectors
    					<md>
    						<mrow>
    							v_1 = \dfrac{v_1}{\|v_1\|} = \dfrac{1}{\sqrt{5}}\vec{1 2 0}
    						</mrow>
    						<mrow>
    							v_2 = \dfrac{v_2}{\|v_2\|} = \dfrac{1}{\sqrt{81}}\vec{6 -3 -6} = \dfrac{1}{3}\vec{2 -1 -2}
    						</mrow>
    						<mrow>
    							v_3 = \dfrac{v_3}{\|v_3\|} = \dfrac{1}{3\sqrt{5}}\vec{4 -2 5}
    						</mrow>
    					</md>
    				</li>
    				<li>If <m>A</m> is <m>m \times n</m> and <m>\dim(\Col(A)) \lt n</m>, then the columns of <m>A</m> are linearly dependent. When we project off previous columns, we are going to end up with a zero vector which is impossible to normalize (without dividing by <m>0</m>).</li>
    				<li>Yes, the columns are linearly independent. As we project off the previous columns (and do linear combinations), we can never end up with zero column.</li>
    			</ol>
    		</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] Suppose that vectors <m>q_1, q_2, \ldots, q_n</m> in <m>\R^m</m> are orthonormal.
    			<ol>
    				<li>Let <m>c_1, c_2, \ldots, c_n</m> be real numbers. What is <m>\|c_1q_1 + c_2q_2 + \cdots + c_nq_n\|^2</m>?</li>
    				<li>Show that <m>q_1, q_2, \ldots, q_n</m> are linearly independent.</li>
    			</ol>
    		</p>
    	</statement>
    	<answer>
    		<p>
    			<ol>
    				<li>Since <m>q_1, q_2, \ldots, q_n</m> are orthonormal, we have <m>q_i \cdot q_i = 1</m> for all <m>i</m> and <m>q_i \cdot q_j = 0</m> for all <m>i \ne j</m>. Thus, we have 
    					<md>
    						<mrow>
    							\|c_1q_1 + c_2q_2 + \cdots + c_nq_n\|^2 = (c_1q_1 + c_2q_2 + \cdots + c_nq_n) \cdot (c_1q_1 + c_2q_2 + \cdots + c_nq_n)
    						</mrow>
    						<mrow>
    							\quad = \sum_{i=1}^n c_i^2 (q_i \cdot q_i) + \sum_{i lt j} 2c_ic_j(q_i \cdot q_j) = \sum_{i=1}^n c_i^2.
    						</mrow>
    					</md>
    				</li>
    				<li>Suppose that there are some real numbers <m>c_1, c_2, \ldots, c_n</m> such that <m>c_1q_1 + c_2q_2 + \cdots + c_nq_n = 0</m>. We have <m>0=\|c_1q_1 + c_2q_2 + \cdots + c_nq_n\|^2=\sum_{i=1}^n c_i^2</m>, which 
    				forces all <m>c_i</m> to be zeroes. Hence, <m>c_1q_1 + c_2q_2 + \cdots + c_nq_n = 0</m> imples <m>c_1=c_2=\cdots=c_n =0</m>, so that <m>q_1, q_2, \ldots, q_n</m> are independent by definition.</li>
    			</ol>
    		</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] Suppose <m>q_1=(c,d,e)</m> and <m>q_2=(f,g,h)</m> are orthonormal column vectors in <m>\R^3</m>. They span a subspace <m>S</m>.
    			<ol>
    				<li>Find the <m>(1,1)</m> entry in the projection matrix <m>P</m> that projects each vector in <m>\R^3</m> onto that subspace <m>S</m>.</li>
    				<li>For this projection matrix <m>P</m>, describe 3 independent eigenvectors (vectors for which <m>Px</m> is a number of <m>\lambda</m> times <m>x</m>). What are the 3 eigenvalues of <m>P</m>? What is its determinant?</li>
    				<li>For some vectors <m>v</m> and <m>w</m> in <m>\R^3</m> the Gram-Schmidt orthonormalization process (applied to <m>v</m> and <m>w</m>)w ill produce those particular vectors
						<m>q_1</m> and <m>q_2</m>. Describe the vectors <m>v</m> and <m>w</m> that lead to this <m>q_1</m> and <m>q_2</m>.</li>
    			</ol>
    		</p>
    	</statement>
    </exercise>
</exercises>
