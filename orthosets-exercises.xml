<?xml version="1.0" encoding="UTF-8"?>
<!--********************************************************************
Copyright 2017 Georgia Institute of Technology

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation.  A copy of
the license is included in gfdl.xml.
*********************************************************************-->
<exercises>
    <exercise>
        <statement>
            <p>Explain why any orthonormal collection of vectors is linearly independent.</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>How does one compute the coordinates of a vector <m>v</m> in an orthonormal basis <m>u_1,\ldots,u_n</m>? Explain why this procedure is generally more efficient than finding coordinates in an non-orthonormal basis. (Caution: any vector in an orthonormal basis <m>u_1, \ldots, u_n</m> has length <m>1</m>, so in the formula for the <m>i</m>th coordinate of <m>v</m> you need not divide by <m>u_i \cdot u_i</m>)</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>Explain step by step how to find an orthonormal basis in a subspace spanned by vectors <m>X_1,\ldots,X_n</m>. (Here <m>X_1,\ldots,X_m</m> need not be linearly independent.)</p>
        </statement>
    </exercise>
    <exercise>
        <statement>
            <p>Every subspace of <m>\R^n</m> admits an orthogonal basis:
                <ol>
                    <li>True</li>
                    <li>False</li>
                </ol>
            </p>
        </statement>
        <answer>
            <p>1 (true). Take any basis, and apply Gram–Schmidt.</p>
        </answer>
    </exercise>
    <exercise>
        <statement>
            <p>Let <m>x</m> and <m>y</m> be nonzero orthogonal vectors in <m>\R^n</m>. Which of the following are true?
                <ol>
                    <li><m>x \cdot y =0</m></li>
                    <li><m>\| x-y \|^2 = \|x\|^2 + \|y\|^2</m></li>
                    <li><m>proj_{\Span(x)}(y)=0</m></li>
                    <li><m>proj_{\Span(y)}(x)=0</m></li>
                </ol>
            </p>
        </statement>
        <answer>
            <p>1,2,3,4.</p>
        </answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] What is the projection of the vector <m>\vec{1 1 1} \in \R^3</m> onto the plane <m>3x-4y+z=0</m>?</p>
    	</statement>
    	<answer>
    		<p>The vector <m>\vec{1 1 1}</m> lies on the plane <m>3x-4y+z=0</m>, so its projection onto that plane is simply <m>\vec{1 1 1}</m> again.</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] Suppose <m>\{v_1, \ldots, v_k\}</m> is an orthonormal set of vectors in <m>\R^n</m>. What happens when you apply Gram-Schmidt process to this set? Why?</p>
    	</statement>
    	<answer>
    		<p>The Gram-Schmidt procedure does not modify orthormal sets of vectors <m>\{v_1, ldots, v_k\}</m> in <m>\R^n</m>. Let us prove this statement by induction on <m>k</m>. If <m>k=1</m>, it is obvious. Consider an orthonormal set <m>\{v_1, \ldots, v_{k+1}\}</m> of <m>k+1</m> vectors in <m>\R^n</m>. We first apply GS to the set <m>\{v_1, \ldots, v_k\}</m>. By the induction 
    		hypothesis, this set stays unchanged. Then we compute to the orthogonal projection of <m>v_{k+1}</m> onto the space spanned by <m>v_1, ldots, v_k</m>. Since <m>v_{k+1}</m> is orthogonal to <m>v_i</m> for <m>i=1 \ldots k</m>, this projection is <m>0</m>. Therefore, GS procedure does not change the set <m>\{v_1, \ldots, v_{k+1}\}</m>.</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW]
    			<ol>
    				<li>Apply Gram-Schmidt to the following vectors in <m>\R^3</m>:
    					<me>\vec{1,2,0}, \vec{8 1 -6}, \vec{0 0 1}</me>
    				</li>
    				<li>Explain why the Gram–Schmidt process always fails (tries to divide by <m>0</m>) on an <m>m \times n</m> matrix <m>A</m> if <m>\dim(\Col(A)) \lt n</m>.</li>
    				<li>Does the Gram–Schmidt process always succeed (never divides by <m>0</m>) if <m>\dim(\Col(A)) = n</m>?</li>
    			</ol>
    		</p>
    	</statement>
    	<answer>
    		<p>
    			<ol>
    				<li>Let
    					<me>x_1=\vec{1 2 0}, x_2=\vec{8 1 -6}, x_3=\vec{0 0 1}</me>
    					Then, 
    					<md>
    						<mrow>
    							v_1 = x_1 = \vec{1 2 0}
    						</mrow>
    						<mrow>
    							v_2 = x_2 - \dfrac{x_2^T v_1}{v_1^T v_1}v_1 = \vec{8 1 -6} - \frac{10}{5}\vec{1 2 0}=\vec{6 -3 -6}
    						</mrow>
    						<mrow>
    							v_3 = x_3 - \dfrac{x_3^T v_1}{v_1^T v_1}v_1 - \dfrac{x_3^T v_2}{v_2^T v_2}v_2= \vec{0 0 1} - 0\vec{1 2 0} - \dfrac{-6}{81}\vec{6 -3 -6} = \dfrac{1}{9}\vec{4 -2 5}
    						</mrow>
    					
    					</md>
    					Now normalize the vectors
    					<md>
    						<mrow>
    							v_1 = \dfrac{v_1}{\|v_1\|} = \dfrac{1}{\sqrt{5}}\vec{1 2 0}
    						</mrow>
    						<mrow>
    							v_2 = \dfrac{v_2}{\|v_2\|} = \dfrac{1}{\sqrt{81}}\vec{6 -3 -6} = \dfrac{1}{3}\vec{2 -1 -2}
    						</mrow>
    						<mrow>
    							v_3 = \dfrac{v_3}{\|v_3\|} = \dfrac{1}{3\sqrt{5}}\vec{4 -2 5}
    						</mrow>
    					</md>
    				</li>
    				<li>If <m>A</m> is <m>m \times n</m> and <m>\dim(\Col(A)) \lt n</m>, then the columns of <m>A</m> are linearly dependent. When we project off previous columns, we are going to end up with a zero vector which is impossible to normalize (without dividing by <m>0</m>).</li>
    				<li>Yes, the columns are linearly independent. As we project off the previous columns (and do linear combinations), we can never end up with zero column.</li>
    			</ol>
    		</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] Suppose that vectors <m>q_1, q_2, \ldots, q_n</m> in <m>\R^m</m> are orthonormal.
    			<ol>
    				<li>Let <m>c_1, c_2, \ldots, c_n</m> be real numbers. What is <m>\|c_1q_1 + c_2q_2 + \cdots + c_nq_n\|^2</m>?</li>
    				<li>Show that <m>q_1, q_2, \ldots, q_n</m> are linearly independent.</li>
    			</ol>
    		</p>
    	</statement>
    	<answer>
    		<p>
    			<ol>
    				<li>Since <m>q_1, q_2, \ldots, q_n</m> are orthonormal, we have <m>q_i \cdot q_i = 1</m> for all <m>i</m> and <m>q_i \cdot q_j = 0</m> for all <m>i \ne j</m>. Thus, we have 
    					<md>
    						<mrow>
    							\|c_1q_1 + c_2q_2 + \cdots + c_nq_n\|^2 = (c_1q_1 + c_2q_2 + \cdots + c_nq_n) \cdot (c_1q_1 + c_2q_2 + \cdots + c_nq_n)
    						</mrow>
    						<mrow>
    							\quad = \sum_{i=1}^n c_i^2 (q_i \cdot q_i) + \sum_{i lt j} 2c_ic_j(q_i \cdot q_j) = \sum_{i=1}^n c_i^2.
    						</mrow>
    					</md>
    				</li>
    				<li>Suppose that there are some real numbers <m>c_1, c_2, \ldots, c_n</m> such that <m>c_1q_1 + c_2q_2 + \cdots + c_nq_n = 0</m>. We have <m>0=\|c_1q_1 + c_2q_2 + \cdots + c_nq_n\|^2=\sum_{i=1}^n c_i^2</m>, which 
    				forces all <m>c_i</m> to be zeroes. Hence, <m>c_1q_1 + c_2q_2 + \cdots + c_nq_n = 0</m> imples <m>c_1=c_2=\cdots=c_n =0</m>, so that <m>q_1, q_2, \ldots, q_n</m> are independent by definition.</li>
    			</ol>
    		</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW] Suppose <m>q_1=(c,d,e)</m> and <m>q_2=(f,g,h)</m> are orthonormal column vectors in <m>\R^3</m>. They span a subspace <m>S</m>.
    			<ol>
    				<li>Find the <m>(1,1)</m> entry in the projection matrix <m>P</m> that projects each vector in <m>\R^3</m> onto that subspace <m>S</m>.</li>
    				<li>For this projection matrix <m>P</m>, describe 3 independent eigenvectors (vectors for which <m>Px</m> is a number of <m>\lambda</m> times <m>x</m>). What are the 3 eigenvalues of <m>P</m>? What is its determinant?</li>
    				<li>For some vectors <m>v</m> and <m>w</m> in <m>\R^3</m> the Gram-Schmidt orthonormalization process (applied to <m>v</m> and <m>w</m>)w ill produce those particular vectors
						<m>q_1</m> and <m>q_2</m>. Describe the vectors <m>v</m> and <m>w</m> that lead to this <m>q_1</m> and <m>q_2</m>.</li>
    			</ol>
    		</p>
    	</statement>
    	<answer>
    		<p>
    			<ol>
    				<li>Denote by <m>Q</m> the matrix with columns <m>q_1</m> and <m>q_2</m>: <m>Q=\mat{c f;d g;e h}</m>. The 
    				projection matrix <m>P = Q(Q^T Q)\inv Q^T</m>. As the column vectors are orthonormal, we know that <m>Q^T Q</m> is the 2-by-2 identity matrix.
    				Thus, <m>P=QQ^T</m> and the first entry is <m>c^2 + f^2</m>.</li>
    				<li>The projection matrix <m>P</m> projects onto a 2d plane. That means its eigenvalues are <m>(1,1,0)</m> and the determinant is <m>0</m>.
    				 The eigenvector corresponding to the eigenvalue <m>0</m> is orthogonal to the projection plane, that is orthogonal to both vectors
    				  <m>q_1</m> and <m>q_2</m>.  The independent vectors corresponding to value 1 are any two independent vectors in the projection plane.  We can choose
    				   <m>q_1</m> and <m>q_2</m> as such vectors.</li>
    				<li>Vector <m>v</m> is on the same line as <m>q_1</m> and in the same direction. Therefore, <m>v=aq_1</m>, where <m>a</m> is a positive number. The second vector
    				 <m>w</m> has to be in the same plane as <m>q_1</m> and <m>q_2</m>, on the same side of the line drawn through <m>q_1</m> as <m>q_2</m> and has to be independent of <m>v</m>.</li>
    			</ol>
    		</p>
    	</answer>
    </exercise>
    <exercise>
    	<statement>
    		<p>[MIT OCW]
    			<ol>
    				<li>If <m>q_1, q_2, q_3</m> are orthonormal vectors in <m>\R^3</m>, what are the possible determinants of this matrix <m>A</m> with columns <m>2q_1, 3q_2, 5q_3</m>? Why?
    					<me>A=\mat{2q_1 3q_2 5q_3}</me>
    				</li>
    				<li>For a matrix <m>A</m>, suppose the cofactor <m>C_{11}</m> of the first entry <m>a_{11}</m> is zero. What information does that give about
 <m>A\inv</m>? Can this inverse exist?</li>
 					<li>Find the 3 eigenvalues of this matrix <m>A</m> and find all of its eigenvectors. Why is the diagonalization <m>S\inv AS=B</m> not possible?
 						<me>A=\mat{2 1 0;0 2 0;0 0 2}</me>
 					</li>
    			</ol>
    		</p>
    	</statement>
    	<answer>
    		<p>
    			<ol>
    				<li>The determinant of the matrix <m>Q=\mat{q_1 q_2 q_3}</m> has to be <m>\pm 1</m>. This is because <m>Q^T Q=I</m> which means that <m>\det(Q^T) \cdot \det(Q)=1</m>, that is, <m>\det(Q^2) = 1</m>.
    				When we multiply a column by a number, the determinant is multiplied by the same number. Thus, the determinant of <m>A</m> is <m>\pm 30</m>.</li>
    				<li>The cofactor <m>C_{11}</m> being zero does not give us enough information to decide whether the inverse exists or not.  For example,  in the matrix
    				 <m>\mat{1 0;0 0}</m> this cofactor is zero and the inverse does not exist, and in the matrix <m>\mat{1 1;1 0}</m> this cofactor is zero and the inverse exists.  If this inverse exists, then we know that the entry <m>(1,1)</m>
    				  in this inverse is zero.</li>
    				<li>The eigenvalues of this matrix are <m>(2,2,2)</m>. But the rank of <m>A-2I</m> is 1. That  means,  you  can  only  find  two  independent  eigenvectors.   When  the
number of independent eigenvectors is smaller than the size of the matrix, then
the diagonalization is not possible because you cannot build the square matrix of
eigenvectors <m>S</m>.</li>
    			</ol>
    		</p>
    	</answer>
    </exercise>
</exercises>
