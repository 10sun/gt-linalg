<?xml version="1.0" encoding="UTF-8"?>

<!--********************************************************************
Copyright 2017 Georgia Institute of Technology

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation.  A copy of
the license is included in gfdl.xml.
*********************************************************************-->

<exercises>
	<exercise>
		<statement>
			<p>Decide whether each subset of  <m>\R^3</m> is linearly dependent or linearly independent.
				<ol>
					<li>
						<me>\left\{\vec{1  -3 5}, \vec{2  2  4}, \vec{4  -4  14} \right\}</me>
					</li>
					<li>
						<me>\left\{\vec{1  7  7}, \vec{2  7  7}, \vec{3  7  7} \right\}</me>
					</li>
					<li>
						<me>\left\{\vec{0  0  -1}, \vec{1  0  4} \right\}</me>
					</li>
					<li>
						<me>\left\{\vec{9  9  0}, \vec{2  0  1}, \vec{3  5  -4}, \vec{12  12  -1} \right\}</me>
					</li>
				</ol>
			</p>
		</statement>
		<answer>
			<p>For each of these, when the subset is independent you must prove it, and when the subset is dependent you must give an example of a dependence.
				<ol>
					<li>It is dependent. Considering
						<me>c_1\vec{1  -3  5} +c_2\vec{2  2  4} + c_3\vec{4  -4  14} =\vec{0  0 0}</me>
						gives this linear system.
						<me>
							\syseq{
							 c_1  +  2c_2  +  4c_3  =  0;
              						-3c_1 +  2c_2  -  4c_3  =  0;
              						5c_1 +  4c_2  +  14c_3 =  0
							}
						</me>
						Row reduction gives
						<md>
							<mrow>
							\amat{
							 1  2  4  0 ;
              						-3 2  -4 0 ;
              						5  4  14 0
							}
							\quad\xrightarrow{}\quad
							&amp;\amat{
							 1  2  4  0 ;
              						0  8  8  0 ;
              						0  0  0  0
							}
							</mrow>
						</md>
						yields a free variable, so there are infinitely many solutions. For an example of a particular dependence we can set <m>c_3</m> to be, say, <m>1</m>.  Then we get <m> c_2=-1</m> and <m>c_1=-2</m>.
					</li>
					<li>It is dependent. The linear system that arises here
						<md>
							<mrow>
							\amat{
							1  2  3  0 ;
              						7  7  7  0 ;
              						7  7  7  0
							}
							\quad\xrightarrow{}\quad
							&amp;\amat{
							1  2  3   0;
              						0  -7 -14 0 ;
              						0  0  0   0
							}
							</mrow>
						</md>
						has infinitely many solutions. We can get a particular solution by taking <m>c_3</m> to be, say, <m>1</m>, and back-substituting to get the resulting <m>c_2</m> and <m>c_1</m>.
					</li>
					<li>It is linearly independent. The system
						<md>
							<mrow>
							\amat{
							  0  1  0;
              						 0  0  0 ;
              						-1 4  0
							}
							\quad\xrightarrow{}\quad
							&amp;\amat{
							-1 4  0 ;
              						0  1  0 ;
              						0  0  0
							}
							</mrow>
						</md>
						has only the solution <m>c_1=0</m> and <m>c_2=0</m>. (We could also have gotten the answer by inspection - the second vector is obviously not a multiple of the first, and vice versa.)
					</li>
					<li>It is linearly dependent. The linear system
						<me>
						\amat{
						9  2  3  12  0 ;
              					9  0  5  12  0 ;
              					0  1  -4 -1  0
						}
						</me>
						has more unknowns than equations, and so row reduction must end with at least one variable free (there can't be a contradictory equation because the system is homogeneous, and so has at least the solution of all zeroes).
          						To exhibit a combination, we can reduce to
          						<me>
          						\amat{
          						 9  2  3  12  0 ;
              					0  -2 2  0   0 ;
              					0  0  -3 -1  0
          						}
          						</me>
          						and take, say,  <m>c_4=1</m>. Then we have that  <m>c_3=-\frac{1}{3}</m>,<m>c_2=-\frac{1}{3}</m>, and <m>c_1=-\frac{31}{27}</m>.
					</li>
				</ol>
			</p>
		</answer>
	</exercise>
	<exercise>
		<statement>
			<p>If <m>A</m> has independent columns, how many solutions does <m>Ax = 0</m> have, and how many solutions can <m>Ax = b</m> have?</p>
		</statement>
	</exercise>
	<exercise>
		<statement>
			<p>Let <m>m > n</m>. Can <m>n</m> vectors span <m>\R^m</m>? Can <m>m</m> vectors in <m>\R^n</m> be linearly independent? Explain.</p>
		</statement>
	</exercise>
	<exercise>
		<statement>
			<p>Let
				<me>A=\mat{1 -3;3 5; -1 7}</me>
				<ol>
					<li>Solve the homogeneous equation <m>Ax=0</m>.</li>
					<li>Based on your answer to (a), determine whether the columns of <m>A</m> are linearly independent.</li>
					<li>Do the columns of <m>A</m> span <m>\R^3</m>?</li>
				</ol>
			</p>
		</statement>
		<answer>
			<p>
				<ol>
					<li>The corresponding augmented matrix
						<md>
							<mrow>
								\mat{1 -3 0; 3 5 0; -1 7 0} \to \mat{1 -3 0;3 5 0;0 4 0} \to \mat{1 -3 0;0 14 0;0 4 0}
							</mrow>
							<mrow>
								\quad \to \mat{1 -3 0;0 1 0;0 4 0} \to \mat{1 -3 0;0 1 0; 0 0 0} \to \mat{1 0 0;0 1 0; 0 0 0}
							</mrow>
						</md>
						The solution is <m>x_1=0=x_2</m>.
					</li>
					<li>Since <m>Ax=0</m> has only the trivial solution, the zero vector can only be the trivial linear combination of the columns of <m>A</m>. Hence the columns of
<m>A</m> are linearly independent.</li>
					<li>No. Since there are only two vectors (columns), the span of two vectors can be at most <m>\R^2</m> so the columns of <m>A</m> do not span <m>\R^3</m>.</li>
				</ol>
			</p>
		</answer>
	</exercise>
	<exercise>
		<statement>
			<p>Build a matrix (choose the numbers m and n and fill in the matrix) to demonstrate the following:
				<ol>
					<li>An <m>m \times n</m> matrix whose columns are lineraly independent but do not span <m>\R^m</m>.</li>
					<li>An <m>m \times n</m> matrix which spans <m>\R^m</m> but whose columns are not linearly independent.</li>
				</ol>
			</p>
		</statement>
		<answer>
			<p>
				<ol>
					<li>
						<me>\mat{1 0;0 1;0 0}</me>
					</li>
					<li>
						<me>\mat{1 0 1;0 1 1}</me>
					</li>
				</ol>
			</p>
		</answer>
	</exercise>
	<exercise>
		<statement>
			<p>Suppose <m>x_3\vec{-8 0 1 0 0 0} + x_5\vec{4 0 0 -3 1 0}</m>, where <m>x_3</m> and <m>x_5</m> are free, is the solution <m>v_h</m> of the homogeneous matrix equation <m>Bx=0</m> for some matrix <m>B</m>.
			 Also, let <m>v_1, v_2, \ldots,</m> be the column vectors of <m>B</m>.
			 	<ol>
			 		<li>You cannot tell from above info how many rows <m>B</m> has. But how many columns must <m>B</m> have and why?</li>
			 		<li>Is the set <m>\{v_1, v_2, \ldots, \}</m> of column vectors of <m>B</m> linearly independent?</li>
			 		<li>Use the equation <m>x_1v_1 + x_2v_2 + \cdots = 0</m> to express <m>v_1</m> as a specific linear combination of the other column vectors or explain why this is impossible.</li>
			 		<li>Express <m>v_2</m> as a linear combination of the other column vectors, or explain why this is impossible.</li>
			 	</ol>
			 </p>
		</statement>
		<answer>
			<p>
				<ol>
					<li>The six rows in the vectors above tell us that the variables are <m>x_1, x_2, \ldots, x_6</m>, one for each column of B. Hence B has 6 columns.</li>
					<li>No. In order to be a linearly independent set, the only solution to <m>x_1v_1 + \ldots + x_6v_6 = 0</m> must be the trivial solution, that is, <m>x_1 = \cdots = x_6 = 0</m>. But since <m>x_3</m> and <m>x_5</m>, this equation has non trivial solutions.</li>
					<li><m>v_h</m> can be written as follows:
						<me>
							x = \vec{x_1 x_2 x_3 x_4 x_5 x_6} = \mat{-8x_3 + 4x_5;0x_2 + 0x_5; x_2; -3x_5; x_5; 0x_3 + 0x_5} = \vec{-8x_3 + 4x_5 0 x_3 -3x_5 x_5 0}
						</me>
						We need special values of <m>x_1, \ldots, x_6</m> which satisfy this equation and where <m>x_1 \ne 0</m>. There are infinitely many ways to do this. For example, let <m>x_3=x_5=1</m>
						 Then we have
						 <me>x_1 = -8\cdot 1 + 4\cdot 1= -4</me>
						 <me>x_2 = 0</me>
						 <me>x_4 = -3\cdot 1</me>
						 <me>x_6 = 0</me>.
						 Thus, <m>-4v_1 + v_3 -3v_4+v_5+v_6=0</m> and we can solve for <m>v_1</m> to get <m>v_1 = 1/4v_3 - 3/4v_4 + 1/4 v_5</m>.
					</li>
					<li>Impossible. If <m>v_2 = \alpha_1 v_1 + \alpha_3 v_3 + \alpha_4 v_4 + \alpha_5 v_5 + \alpha_6 v_6</m> then <m>0=\alpha_1 v_1+(-1)v_2 + \alpha_3 v_3 + \cdots + \alpha_6 v_6</m>, a solution of <m>Bx=0</m> in which <m>x_2 \ne 0</m>.
					 But <m>v_h</m> represents all solutions of <m>Bx=0</m>, and <m>v_h</m> has a zero in the second row. i.e., <m>x_2</m> is always 0.</li>
				</ol>
			</p>
		</answer>
	</exercise>
	<exercise>
		<statement>
			<p>We already know that the linear independence among the columns of a matrix is preserved by column operations. Is the linear independence among the columns preserved by row operations?
			</p>
		</statement>
		<answer>
			<p>Suppose a matrix <m>A</m> is changed to a matrix <m>B</m> by row operations. Then the same operations change <m>\mat{A 0}</m> to <m>\mat{B 0}</m>. Therefore the solutions of the homogeneous systems <m>Ax = 0</m> and <m>Bx = 0</m> are the same. In particular,
			we have <m>Ax = 0</m> has only the trivial solution <m>\Leftrightarrow Bx = 0</m> has only the trivial solution. In other words, the uniqueness with coefficient matrix <m>A</m> is equivalent to the uniqueness with coefficient matrix <m>B</m>. Since the uniqueness is equivalent to the linear independence of the columns, we conclude that
			column of <m>A</m> linearly independent <m>\Leftrightarrow</m> column of <m>B</m> linearly independent.
			</p>
		</answer>
	</exercise>
	<exercise>
		<statement>
			<p>Prove that for a subspace <m>H</m> spanned by any given vectors, <m>H</m> is also spanned by the maximal linearly independent vectors.</p>
		</statement>
		<answer>
			<p>Let <m>u, v, w</m> be maximal linearly independent among <m>u, v, w, x, y</m>. Then the maximality means that by adding <m>x</m> to the collection, the four vectors <m>u, v, w, x</m> are linearly dependent. Thus we have <m>a, b, c, d,</m> not all zero, such that
				<me>au + bv + cw + dx = 0</me>.
				If <m>d = 0</m>, then <m>au + bv + cw = 0</m> and <m>a, b, c</m> are not all zero. This contradicts with the assumption that <m>u, v, w </m> are linearly independent. Therefore we have <m>d \ne 0</m>, and
				<me>x = (-a/d)u + (-b/d)v + (-c/d)w \in \Span\{u, v, w\}</me>.
				Similarly, we have <m>y \in \Span\{u, v, w\}</m>. Then, we have
				<me>\Span\{u, v, w, x, y\} = \Span\{u, v, w\}</me>.
				This finishes the proof in case 3 out of 5 vectors are maximal linearly independent. The general case is similar.
			</p>
		</answer>
	</exercise>
</exercises>
